{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据读取与分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/data/data103654\n"
     ]
    }
   ],
   "source": [
    "# 进入比赛数据集存放目录\n",
    "%cd /home/aistudio/data/data103654/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用pandas读取数据集\n",
    "import pandas as pd\n",
    "train = pd.read_table('train.txt', sep='\\t',header=None)  # 训练集\n",
    "dev = pd.read_table('dev.txt', sep='\\t',header=None)      # 验证集（官方已经划分的）\n",
    "test = pd.read_table('test.txt', sep='\\t',header=None)    # 测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>网易第三季度业绩低于分析师预期</td>\n",
       "      <td>科技</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>巴萨1年前地狱重现这次却是天堂 再赴魔鬼客场必翻盘</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>美国称支持向朝鲜提供紧急人道主义援助</td>\n",
       "      <td>时政</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>增资交银康联 交行夺参股险商首单</td>\n",
       "      <td>股票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>午盘：原材料板块领涨大盘</td>\n",
       "      <td>股票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752466</th>\n",
       "      <td>天津女排奇迹之源竟在场边 他是五冠王真正核心</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752467</th>\n",
       "      <td>北电网络专利拍卖推迟：可能分拆6部分拍卖</td>\n",
       "      <td>科技</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752468</th>\n",
       "      <td>Spirit AeroSystems债券发行价确定</td>\n",
       "      <td>股票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752469</th>\n",
       "      <td>陆慧明必发火线：法兰克福无胜 曼联国米顺利过关</td>\n",
       "      <td>彩票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752470</th>\n",
       "      <td>首破万元 索尼46寸全新LED液晶特价促</td>\n",
       "      <td>科技</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>752471 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0   1\n",
       "0                 网易第三季度业绩低于分析师预期  科技\n",
       "1       巴萨1年前地狱重现这次却是天堂 再赴魔鬼客场必翻盘  体育\n",
       "2              美国称支持向朝鲜提供紧急人道主义援助  时政\n",
       "3                增资交银康联 交行夺参股险商首单  股票\n",
       "4                    午盘：原材料板块领涨大盘  股票\n",
       "...                           ...  ..\n",
       "752466     天津女排奇迹之源竟在场边 他是五冠王真正核心  体育\n",
       "752467       北电网络专利拍卖推迟：可能分拆6部分拍卖  科技\n",
       "752468  Spirit AeroSystems债券发行价确定  股票\n",
       "752469    陆慧明必发火线：法兰克福无胜 曼联国米顺利过关  彩票\n",
       "752470       首破万元 索尼46寸全新LED液晶特价促  科技\n",
       "\n",
       "[752471 rows x 2 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取伪标签数据（将模型对无标签的测试集的预测结果加入到训练中去，具体可以看前面上分之路的说明）\n",
    "# newtest.csv对应第一轮伪标签,newtest1.csv对应第二轮伪标签，newtest2.csv对应第三轮伪标签，newtest3.csv对应第四轮伪标签。 第一轮使用时提升较大，后面提升逐渐微弱\n",
    "newtest = pd.read_csv('/home/aistudio/work/newtest3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于数据集存放时无列名，因此手动添加列名便于对数据进行更好处理\n",
    "train.columns = [\"text_a\",'label']\n",
    "dev.columns = [\"text_a\",'label']\n",
    "test.columns = [\"text_a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>网易第三季度业绩低于分析师预期</td>\n",
       "      <td>科技</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>巴萨1年前地狱重现这次却是天堂 再赴魔鬼客场必翻盘</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>美国称支持向朝鲜提供紧急人道主义援助</td>\n",
       "      <td>时政</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>增资交银康联 交行夺参股险商首单</td>\n",
       "      <td>股票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>午盘：原材料板块领涨大盘</td>\n",
       "      <td>股票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752466</th>\n",
       "      <td>天津女排奇迹之源竟在场边 他是五冠王真正核心</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752467</th>\n",
       "      <td>北电网络专利拍卖推迟：可能分拆6部分拍卖</td>\n",
       "      <td>科技</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752468</th>\n",
       "      <td>Spirit AeroSystems债券发行价确定</td>\n",
       "      <td>股票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752469</th>\n",
       "      <td>陆慧明必发火线：法兰克福无胜 曼联国米顺利过关</td>\n",
       "      <td>彩票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752470</th>\n",
       "      <td>首破万元 索尼46寸全新LED液晶特价促</td>\n",
       "      <td>科技</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>752471 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           text_a label\n",
       "0                 网易第三季度业绩低于分析师预期    科技\n",
       "1       巴萨1年前地狱重现这次却是天堂 再赴魔鬼客场必翻盘    体育\n",
       "2              美国称支持向朝鲜提供紧急人道主义援助    时政\n",
       "3                增资交银康联 交行夺参股险商首单    股票\n",
       "4                    午盘：原材料板块领涨大盘    股票\n",
       "...                           ...   ...\n",
       "752466     天津女排奇迹之源竟在场边 他是五冠王真正核心    体育\n",
       "752467       北电网络专利拍卖推迟：可能分拆6部分拍卖    科技\n",
       "752468  Spirit AeroSystems债券发行价确定    股票\n",
       "752469    陆慧明必发火线：法兰克福无胜 曼联国米顺利过关    彩票\n",
       "752470       首破万元 索尼46寸全新LED液晶特价促    科技\n",
       "\n",
       "[752471 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看训练集数据，共752471条\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>网民市民集体幻想中奖后如果你中了9000万怎么办</td>\n",
       "      <td>彩票</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PVC期货有望5月挂牌</td>\n",
       "      <td>财经</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>午时三刻新作《幻神录―宿命情缘》</td>\n",
       "      <td>游戏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>欧司朗LLFY网络提供一站式照明解决方案</td>\n",
       "      <td>家居</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>试探北京楼市向何方：排不完的队　涨不够的价</td>\n",
       "      <td>房产</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>王大雷看国足比赛预测比分我觉得是2-0或者3-1</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>克雷扎回归猛龙势如破竹希尔遭驱逐太阳惨败51分</td>\n",
       "      <td>体育</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>王建宙将与台商共创4G网络商机</td>\n",
       "      <td>科技</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>普京突访食品超市做调查不满高价猪肉(图)</td>\n",
       "      <td>时政</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>高空俯视女明星性感乳沟(组图)(7)</td>\n",
       "      <td>时尚</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         text_a label\n",
       "0      网民市民集体幻想中奖后如果你中了9000万怎么办    彩票\n",
       "1                   PVC期货有望5月挂牌    财经\n",
       "2              午时三刻新作《幻神录―宿命情缘》    游戏\n",
       "3          欧司朗LLFY网络提供一站式照明解决方案    家居\n",
       "4         试探北京楼市向何方：排不完的队　涨不够的价    房产\n",
       "...                         ...   ...\n",
       "79995  王大雷看国足比赛预测比分我觉得是2-0或者3-1    体育\n",
       "79996   克雷扎回归猛龙势如破竹希尔遭驱逐太阳惨败51分    体育\n",
       "79997           王建宙将与台商共创4G网络商机    科技\n",
       "79998      普京突访食品超市做调查不满高价猪肉(图)    时政\n",
       "79999        高空俯视女明星性感乳沟(组图)(7)    时尚\n",
       "\n",
       "[80000 rows x 2 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看验证集数据，共80000条\n",
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>北京君太百货璀璨秋色 满100省353020元</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>教育部：小学高年级将开始学习性知识</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>专业级单反相机 佳能7D单机售价9280元</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>星展银行起诉内地客户 银行强硬客户无奈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>脱离中国的实际 强压人民币大幅升值只能是梦想</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83594</th>\n",
       "      <td>Razer杯DotA精英挑战赛8月震撼登场</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83595</th>\n",
       "      <td>经济数据好转吹散人民币贬值预期</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83596</th>\n",
       "      <td>抵押率抵押物双控政策 刘明康支招房产贷款</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83597</th>\n",
       "      <td>8000万像素 利图发布Aptus-II 12数码后背</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83598</th>\n",
       "      <td>教育部公布33个国家万余所正规学校名单</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83599 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text_a\n",
       "0          北京君太百货璀璨秋色 满100省353020元\n",
       "1                教育部：小学高年级将开始学习性知识\n",
       "2            专业级单反相机 佳能7D单机售价9280元\n",
       "3              星展银行起诉内地客户 银行强硬客户无奈\n",
       "4           脱离中国的实际 强压人民币大幅升值只能是梦想\n",
       "...                            ...\n",
       "83594        Razer杯DotA精英挑战赛8月震撼登场\n",
       "83595              经济数据好转吹散人民币贬值预期\n",
       "83596         抵押率抵押物双控政策 刘明康支招房产贷款\n",
       "83597  8000万像素 利图发布Aptus-II 12数码后背\n",
       "83598          教育部公布33个国家万余所正规学校名单\n",
       "\n",
       "[83599 rows x 1 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看测试集数据，共83599条\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拼接训练和验证集，便于进行统计分析\n",
    "total = pd.concat([train,dev],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "科技    162245\n",
       "股票    153949\n",
       "体育    130982\n",
       "娱乐     92228\n",
       "时政     62867\n",
       "社会     50541\n",
       "教育     41680\n",
       "财经     36963\n",
       "家居     32363\n",
       "游戏     24283\n",
       "房产     19922\n",
       "时尚     13335\n",
       "彩票      7598\n",
       "星座      3515\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 总类别标签分布统计\n",
    "total['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    832471.000000\n",
       "mean         19.388112\n",
       "std           4.097139\n",
       "min           2.000000\n",
       "25%          17.000000\n",
       "50%          20.000000\n",
       "75%          23.000000\n",
       "max          48.000000\n",
       "Name: text_a, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文本长度统计分析,通过分析可以看出文本较短，最长为48\n",
    "total['text_a'].map(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    83599.000000\n",
       "mean        19.815022\n",
       "std          3.883845\n",
       "min          3.000000\n",
       "25%         17.000000\n",
       "50%         20.000000\n",
       "75%         23.000000\n",
       "max         84.000000\n",
       "Name: text_a, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对测试集的长度统计分析，可以看出在长度上分布与训练数据相近\n",
    "test['text_a'].map(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拼接训练集和伪标签数据，通过加入伪标签数据，增大训练数据量提升模型泛化能力\n",
    "train = pd.concat([train,newtest],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存处理后的数据集文件\n",
    "train.to_csv('train.csv', sep='\\t', index=False)  # 保存训练集，格式为text_a,label，以\\t分隔开\n",
    "dev.to_csv('dev.csv', sep='\\t', index=False)      # 保存验证集，格式为text_a,label，以\\t分隔开\n",
    "test.to_csv('test.csv', sep='\\t', index=False)    # 保存测试集，格式为text_a，以\\t分隔开"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于PaddleNLP构建基线模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前置环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的第三方库\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "from functools import partial\n",
    "import random\n",
    "import time\n",
    "import inspect\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from paddle.io import IterableDataset\n",
    "from paddle.utils.download import get_path_from_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: paddlenlp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.1.1)\n",
      "Collecting paddlenlp\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a5/de/3f20df026e48eae755ea06cbd587dd845767ac2d04e3bcf5e24cdb62cc4f/paddlenlp-2.5.0-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fastapi\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8f/89/adf4525d1870021b65ec562e83e9f46d96494ad95f238d0264ef1ab6b604/fastapi-0.89.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.64.1)\n",
      "Requirement already satisfied: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied: paddlefsl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.0.0)\n",
      "Requirement already satisfied: paddle2onnx in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.0.0)\n",
      "Requirement already satisfied: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.4.0)\n",
      "Collecting datasets>=2.0.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/24/57/6b07e4dc51479ae3e9bbc774af348b0307e2b66957ceae94d25e3f9d7dcf/datasets-2.8.0-py3-none-any.whl (452 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uvicorn\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/96/f3/f39ac8ac3bdf356b4934b8f7e56173e96681f67ef0cd92bd33a5059fae9e/uvicorn-0.20.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.1.96)\n",
      "Requirement already satisfied: rich in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (12.6.0)\n",
      "Requirement already satisfied: dill<0.3.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.3.3)\n",
      "Collecting huggingface-hub>=0.11.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a0/0b/e4c1165bb954036551e61e1d7858e3293347f360d8f84854092f3ad38446/huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<=3.20.0,>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (3.20.0)\n",
      "Requirement already satisfied: multiprocess<=0.70.12.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\n",
      "Requirement already satisfied: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Collecting typer\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/0d/44/56c3f48d2bb83d76f5c970aef8e2c3ebd6a832f09e3621c5395371fe6999/typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting responses<0.19\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/79/f3/2b3a6dc5986303b3dd1bbbcf482022acb2583c428cd23f0b6d37b1a1a519/responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (1.1.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (21.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (10.0.1)\n",
      "Collecting xxhash\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7f/9f/8645235cc0913d54eb38599e9bfbd884de6f430cd1a3217530ccb6cc1800/xxhash-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.1/213.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (2.24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (5.1.2)\n",
      "Collecting aiohttp\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7a/48/7882af39221fee58e33eee6c8e516097e2331334a5937f54fe5b5b285d9e/aiohttp-3.8.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (948 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.0/948.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp) (1.19.5)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/37/57/eb7c3c10b187d3b8565946772ce0229c79e3c623010eda0aeb5032ff56f4/fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.5/139.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from huggingface-hub>=0.11.1->paddlenlp) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from huggingface-hub>=0.11.1->paddlenlp) (4.3.0)\n",
      "Collecting starlette==0.22.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1d/4e/30eda84159d5b3ad7fe663c40c49b16dd17436abe838f10a56c34bee44e8/starlette-0.22.0-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6f/6a/a3b9a51b886eeee570ddb32ae64a8d2fd00cd25cb1daaf82260188d2d1e4/pydantic-1.10.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from starlette==0.22.0->fastapi->paddlenlp) (3.6.1)\n",
      "Requirement already satisfied: pillow==8.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlefsl->paddlenlp) (8.2.0)\n",
      "Collecting paddlefsl\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fb/4a/25d1959a8f1fe5ee400f32fc9fc8b56d4fd6fc25315e23c0171f6e705e2a/paddlefsl-1.1.0-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.0/101.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rich->paddlenlp) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rich->paddlenlp) (2.13.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from typer->paddlenlp) (8.0.4)\n",
      "Collecting h11>=0.8\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.16.0)\n",
      "Requirement already satisfied: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.2.3)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (3.0.0)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp) (22.1.0)\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e8/b6/8d17e169d577ca7678b11cd0d3ceebb0a6089a7f4a2de4b945fe4b1c86db/asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d6/c1/8991e7c5385b897b8c020cdaad718c5b087a6626d1d11a23e1ea87e325a7/async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/02/c6/13751bb69244e4835fa8192a20d1d1110091f717f1e1b0168320ed1a29c9/yarl-1.8.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.4/231.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/47/e4/745fb4cc79b439b1c1d1f441f2aa65f6250b77052d2bf4d8d8b5970ee672/multidict-6.0.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b2/18/3b0eb2690b3bf4d340a221d0e76b6c5f4cac9d5dd37fb8c7b6ec25c2f510/frozenlist-1.3.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.0/148.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/db/51/a507c856293ab05cdc1db77ff4bc1268ddd39f29e7dc4919aa497f0adbec/charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from packaging->datasets>=2.0.0->paddlenlp) (3.0.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp) (2019.9.11)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\n",
      "Requirement already satisfied: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata->datasets>=2.0.0->paddlenlp) (3.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp) (2.0.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->visualdl->paddlenlp) (56.2.0)\n",
      "Installing collected packages: xxhash, urllib3, pydantic, multidict, h11, fsspec, frozenlist, charset-normalizer, asynctest, async-timeout, yarl, starlette, aiosignal, uvicorn, typer, responses, paddlefsl, huggingface-hub, fastapi, aiohttp, datasets, paddlenlp\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.6\n",
      "    Uninstalling urllib3-1.25.6:\n",
      "      Successfully uninstalled urllib3-1.25.6\n",
      "  Attempting uninstall: paddlefsl\n",
      "    Found existing installation: paddlefsl 1.0.0\n",
      "    Uninstalling paddlefsl-1.0.0:\n",
      "      Successfully uninstalled paddlefsl-1.0.0\n",
      "  Attempting uninstall: paddlenlp\n",
      "    Found existing installation: paddlenlp 2.1.1\n",
      "    Uninstalling paddlenlp-2.1.1:\n",
      "      Successfully uninstalled paddlenlp-2.1.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "parl 1.4.1 requires pyzmq==18.1.1, but you have pyzmq 23.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohttp-3.8.3 aiosignal-1.3.1 async-timeout-4.0.2 asynctest-0.13.0 charset-normalizer-2.1.1 datasets-2.8.0 fastapi-0.89.1 frozenlist-1.3.3 fsspec-2022.11.0 h11-0.14.0 huggingface-hub-0.11.1 multidict-6.0.4 paddlefsl-1.1.0 paddlenlp-2.5.0 pydantic-1.10.4 responses-0.18.0 starlette-0.22.0 typer-0.7.0 urllib3-1.25.11 uvicorn-0.20.0 xxhash-3.2.0 yarl-1.8.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 下载最新版本的paddlenlp\n",
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入paddlenlp所需的相关包\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddle.dataset.common import md5file\n",
    "from paddlenlp.datasets import DatasetBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义要进行微调的预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-19 10:21:26,629] [    INFO] - Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"fuse\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"paddlenlp_version\": null,\n",
      "  \"pool_act\": \"tanh\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "[2023-01-19 10:21:26,635] [    INFO] - Configuration saved in /home/aistudio/.paddlenlp/models/bert-wwm-ext-chinese/config.json\n",
      "[2023-01-19 10:21:26,638] [    INFO] - Downloading bert-wwm-ext-chinese.pdparams from http://bj.bcebos.com/paddlenlp/models/transformers/bert/bert-wwm-ext-chinese.pdparams\n",
      "100%|██████████| 390M/390M [00:14<00:00, 28.1MB/s] \n",
      "W0119 10:21:43.192117    98 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2\n",
      "W0119 10:21:43.195968    98 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\n",
      "[2023-01-19 10:21:43,991] [    INFO] - All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "[2023-01-19 10:21:43,995] [ WARNING] - Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-wwm-ext-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2023-01-19 10:21:43,998] [    INFO] - Downloading http://bj.bcebos.com/paddlenlp/models/transformers/bert/bert-wwm-ext-chinese-vocab.txt and saved to /home/aistudio/.paddlenlp/models/bert-wwm-ext-chinese\n",
      "[2023-01-19 10:21:44,036] [    INFO] - Downloading bert-wwm-ext-chinese-vocab.txt from http://bj.bcebos.com/paddlenlp/models/transformers/bert/bert-wwm-ext-chinese-vocab.txt\n",
      "100%|██████████| 107k/107k [00:00<00:00, 5.75MB/s]\n",
      "[2023-01-19 10:21:44,150] [    INFO] - tokenizer config file saved in /home/aistudio/.paddlenlp/models/bert-wwm-ext-chinese/tokenizer_config.json\n",
      "[2023-01-19 10:21:44,152] [    INFO] - Special tokens file saved in /home/aistudio/.paddlenlp/models/bert-wwm-ext-chinese/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# Bert模型尝鲜\n",
    "MODEL_NAME = \"bert-wwm-ext-chinese\"\n",
    "#只需指定想要使用的模型名称和文本分类的类别数即可完成Fine-tune网络定义，通过在预训练模型后拼接上一个全连接网络（Full Connected）进行分类\n",
    "model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=14) # 此次分类任务为14分类任务，故num_classes设置为14\n",
    "#定义模型对应的tokenizer，tokenizer可以把原始输入文本转化成模型model可接受的输入数据格式。需注意tokenizer类要与选择的模型相对应，具体可以查看PaddleNLP相关文档\n",
    "tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PaddleNLP还支持其他预训练模型。具体可以查看：[PaddleNLP模型汇总](https://paddlenlp.readthedocs.io/zh/latest/model_zoo/index.html#transformer) 、[ Transformer类](https://paddlenlp.readthedocs.io/zh/latest/source/paddlenlp.transformers.html)\n",
    "更多支持模型请查看：[PaddleNLP Transformer预训练模式](https://paddlenlp.readthedocs.io/zh/latest/model_zoo/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取和处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['科技', '体育', '时政', '股票', '娱乐', '教育', '家居', '财经', '房产', '社会', '游戏', '彩票', '星座', '时尚']\n"
     ]
    }
   ],
   "source": [
    "# 定义要进行分类的14个类别\n",
    "label_list=list(train.label.unique())\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集对应文件及其文件存储格式\n",
    "class NewsData(DatasetBuilder):\n",
    "    SPLITS = {\n",
    "        'train': 'train.csv',  # 训练集\n",
    "        'dev': 'dev.csv',      # 验证集\n",
    "    }\n",
    "\n",
    "    def _get_data(self, mode, **kwargs):\n",
    "        filename = self.SPLITS[mode]\n",
    "        return filename\n",
    "\n",
    "    def _read(self, filename):\n",
    "        \"\"\"读取数据\"\"\"\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            head = None\n",
    "            for line in f:\n",
    "                data = line.strip().split(\"\\t\")    # 以'\\t'分隔各列\n",
    "                if not head:\n",
    "                    head = data\n",
    "                else:\n",
    "                    text_a, label = data\n",
    "                    yield {\"text_a\": text_a, \"label\": label}  # 此次设置数据的格式为：text_a,label，可以根据具体情况进行修改\n",
    "\n",
    "    def get_labels(self):\n",
    "        return label_list   # 类别标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集加载函数\n",
    "def load_dataset(name=None,\n",
    "                 data_files=None,\n",
    "                 splits=None,\n",
    "                 lazy=None,\n",
    "                 **kwargs):\n",
    "   \n",
    "    reader_cls = NewsData  # 加载定义的数据集格式\n",
    "    print(reader_cls)\n",
    "    if not name:\n",
    "        reader_instance = reader_cls(lazy=lazy, **kwargs)\n",
    "    else:\n",
    "        reader_instance = reader_cls(lazy=lazy, name=name, **kwargs)\n",
    "\n",
    "    datasets = reader_instance.read_datasets(data_files=data_files, splits=splits)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.NewsData'>\n"
     ]
    }
   ],
   "source": [
    "# 加载训练和验证集\n",
    "train_ds, dev_ds = load_dataset(splits=[\"train\", \"dev\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据加载和处理函数\n",
    "def convert_example(example, tokenizer, max_seq_length=128, is_test=False):\n",
    "    qtconcat = example[\"text_a\"]\n",
    "    encoded_inputs = tokenizer(text=qtconcat, max_seq_len=max_seq_length)  # tokenizer处理为模型可接受的格式 \n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids\n",
    "\n",
    "# 定义数据加载函数dataloader\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    # 训练数据集随机打乱，测试数据集不打乱\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置：\n",
    "# 批处理大小，显存如若不足的话可以适当改小该值\n",
    "batch_size = 600\n",
    "# 文本序列最大截断长度，需要根据文本具体长度进行确定，最长不超过512。 通过文本长度分析可以看出文本长度最大为48，故此处设置为48\n",
    "max_seq_length = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据处理成模型可读入的数据格式\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack()  # labels\n",
    "): [data for data in fn(samples)]\n",
    "\n",
    "# 训练集迭代器\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "\n",
    "# 验证集迭代器\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置Fine-Tune优化策略\n",
    "适用于BERT这类Transformer模型的学习率为warmup的动态学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参，loss，优化器等\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "# 定义训练配置参数：\n",
    "# 定义训练过程中的最大学习率\n",
    "learning_rate = 4e-5\n",
    "# 训练轮次\n",
    "epochs = 5\n",
    "# 学习率预热比例\n",
    "warmup_proportion = 0.1\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\n",
    "weight_decay = 0.0\n",
    "\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "\n",
    "# AdamW优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])\n",
    "\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()  # 交叉熵损失函数\n",
    "metric = paddle.metric.Accuracy()              # accuracy评价指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型训练验证评估函数\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))  # 输出验证集上评估效果\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return accu  # 返回准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<paddle.fluid.core_avx.Generator at 0x7f16e93d15f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 固定随机种子便于结果的复现\n",
    "seed = 1024\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "paddle.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重新开始\n",
      "global step 10, epoch: 1, batch: 10, loss: 2.66119, acc: 0.07833\n",
      "global step 20, epoch: 1, batch: 20, loss: 2.59639, acc: 0.08025\n",
      "global step 30, epoch: 1, batch: 30, loss: 2.51010, acc: 0.09061\n",
      "global step 40, epoch: 1, batch: 40, loss: 2.38781, acc: 0.11958\n",
      "global step 50, epoch: 1, batch: 50, loss: 2.24122, acc: 0.15933\n",
      "global step 60, epoch: 1, batch: 60, loss: 2.09791, acc: 0.19925\n",
      "global step 70, epoch: 1, batch: 70, loss: 1.85593, acc: 0.24750\n",
      "global step 80, epoch: 1, batch: 80, loss: 1.63663, acc: 0.29465\n",
      "global step 90, epoch: 1, batch: 90, loss: 1.37212, acc: 0.33863\n",
      "global step 100, epoch: 1, batch: 100, loss: 1.19518, acc: 0.37883\n",
      "global step 110, epoch: 1, batch: 110, loss: 1.03339, acc: 0.41418\n",
      "global step 120, epoch: 1, batch: 120, loss: 0.86965, acc: 0.44732\n",
      "global step 130, epoch: 1, batch: 130, loss: 0.77490, acc: 0.47626\n",
      "global step 140, epoch: 1, batch: 140, loss: 0.65116, acc: 0.50190\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.60662, acc: 0.52543\n",
      "global step 160, epoch: 1, batch: 160, loss: 0.49232, acc: 0.54759\n",
      "global step 170, epoch: 1, batch: 170, loss: 0.48146, acc: 0.56703\n",
      "global step 180, epoch: 1, batch: 180, loss: 0.44777, acc: 0.58525\n",
      "global step 190, epoch: 1, batch: 190, loss: 0.41675, acc: 0.60175\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.39443, acc: 0.61667\n",
      "global step 210, epoch: 1, batch: 210, loss: 0.34643, acc: 0.63054\n",
      "global step 220, epoch: 1, batch: 220, loss: 0.35308, acc: 0.64328\n",
      "global step 230, epoch: 1, batch: 230, loss: 0.36376, acc: 0.65492\n",
      "global step 240, epoch: 1, batch: 240, loss: 0.31589, acc: 0.66587\n",
      "global step 250, epoch: 1, batch: 250, loss: 0.31602, acc: 0.67606\n",
      "global step 260, epoch: 1, batch: 260, loss: 0.27032, acc: 0.68562\n",
      "global step 270, epoch: 1, batch: 270, loss: 0.31550, acc: 0.69436\n",
      "global step 280, epoch: 1, batch: 280, loss: 0.30307, acc: 0.70254\n",
      "global step 290, epoch: 1, batch: 290, loss: 0.33737, acc: 0.71020\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.29904, acc: 0.71731\n",
      "global step 310, epoch: 1, batch: 310, loss: 0.26518, acc: 0.72398\n",
      "global step 320, epoch: 1, batch: 320, loss: 0.30700, acc: 0.73021\n",
      "global step 330, epoch: 1, batch: 330, loss: 0.34298, acc: 0.73618\n",
      "global step 340, epoch: 1, batch: 340, loss: 0.23530, acc: 0.74180\n",
      "global step 350, epoch: 1, batch: 350, loss: 0.23906, acc: 0.74720\n",
      "global step 360, epoch: 1, batch: 360, loss: 0.27780, acc: 0.75223\n",
      "global step 370, epoch: 1, batch: 370, loss: 0.30557, acc: 0.75689\n",
      "global step 380, epoch: 1, batch: 380, loss: 0.23961, acc: 0.76147\n",
      "global step 390, epoch: 1, batch: 390, loss: 0.25981, acc: 0.76582\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.27443, acc: 0.76995\n",
      "global step 410, epoch: 1, batch: 410, loss: 0.22395, acc: 0.77381\n",
      "global step 420, epoch: 1, batch: 420, loss: 0.29966, acc: 0.77743\n",
      "global step 430, epoch: 1, batch: 430, loss: 0.23348, acc: 0.78105\n",
      "global step 440, epoch: 1, batch: 440, loss: 0.24400, acc: 0.78445\n",
      "global step 450, epoch: 1, batch: 450, loss: 0.28540, acc: 0.78779\n",
      "global step 460, epoch: 1, batch: 460, loss: 0.18789, acc: 0.79092\n",
      "global step 470, epoch: 1, batch: 470, loss: 0.27304, acc: 0.79394\n",
      "global step 480, epoch: 1, batch: 480, loss: 0.21119, acc: 0.79694\n",
      "global step 490, epoch: 1, batch: 490, loss: 0.22885, acc: 0.79981\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.22951, acc: 0.80259\n",
      "global step 510, epoch: 1, batch: 510, loss: 0.21052, acc: 0.80511\n",
      "global step 520, epoch: 1, batch: 520, loss: 0.21474, acc: 0.80763\n",
      "global step 530, epoch: 1, batch: 530, loss: 0.19603, acc: 0.81002\n",
      "global step 540, epoch: 1, batch: 540, loss: 0.22670, acc: 0.81242\n",
      "global step 550, epoch: 1, batch: 550, loss: 0.25116, acc: 0.81459\n",
      "global step 560, epoch: 1, batch: 560, loss: 0.22971, acc: 0.81674\n",
      "global step 570, epoch: 1, batch: 570, loss: 0.21045, acc: 0.81880\n",
      "global step 580, epoch: 1, batch: 580, loss: 0.24753, acc: 0.82089\n",
      "global step 590, epoch: 1, batch: 590, loss: 0.19074, acc: 0.82291\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.21842, acc: 0.82484\n",
      "global step 610, epoch: 1, batch: 610, loss: 0.23195, acc: 0.82665\n",
      "global step 620, epoch: 1, batch: 620, loss: 0.26410, acc: 0.82843\n",
      "global step 630, epoch: 1, batch: 630, loss: 0.20664, acc: 0.83026\n",
      "global step 640, epoch: 1, batch: 640, loss: 0.21169, acc: 0.83202\n",
      "global step 650, epoch: 1, batch: 650, loss: 0.23543, acc: 0.83370\n",
      "global step 660, epoch: 1, batch: 660, loss: 0.25341, acc: 0.83527\n",
      "global step 670, epoch: 1, batch: 670, loss: 0.17608, acc: 0.83686\n",
      "global step 680, epoch: 1, batch: 680, loss: 0.20227, acc: 0.83835\n",
      "global step 690, epoch: 1, batch: 690, loss: 0.23098, acc: 0.83974\n",
      "global step 700, epoch: 1, batch: 700, loss: 0.19576, acc: 0.84120\n",
      "global step 710, epoch: 1, batch: 710, loss: 0.14164, acc: 0.84260\n",
      "global step 720, epoch: 1, batch: 720, loss: 0.14374, acc: 0.84394\n",
      "global step 730, epoch: 1, batch: 730, loss: 0.19447, acc: 0.84524\n",
      "global step 740, epoch: 1, batch: 740, loss: 0.18822, acc: 0.84659\n",
      "global step 750, epoch: 1, batch: 750, loss: 0.19005, acc: 0.84792\n",
      "global step 760, epoch: 1, batch: 760, loss: 0.15028, acc: 0.84913\n",
      "global step 770, epoch: 1, batch: 770, loss: 0.20757, acc: 0.85029\n",
      "global step 780, epoch: 1, batch: 780, loss: 0.17515, acc: 0.85143\n",
      "global step 790, epoch: 1, batch: 790, loss: 0.14851, acc: 0.85254\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.20328, acc: 0.85368\n",
      "global step 810, epoch: 1, batch: 810, loss: 0.16211, acc: 0.85476\n",
      "global step 820, epoch: 1, batch: 820, loss: 0.15148, acc: 0.85586\n",
      "global step 830, epoch: 1, batch: 830, loss: 0.14248, acc: 0.85700\n",
      "global step 840, epoch: 1, batch: 840, loss: 0.16863, acc: 0.85803\n",
      "global step 850, epoch: 1, batch: 850, loss: 0.18701, acc: 0.85904\n",
      "global step 860, epoch: 1, batch: 860, loss: 0.20701, acc: 0.86002\n",
      "global step 870, epoch: 1, batch: 870, loss: 0.17993, acc: 0.86091\n",
      "global step 880, epoch: 1, batch: 880, loss: 0.19168, acc: 0.86188\n",
      "global step 890, epoch: 1, batch: 890, loss: 0.17815, acc: 0.86283\n",
      "global step 900, epoch: 1, batch: 900, loss: 0.14611, acc: 0.86378\n",
      "global step 910, epoch: 1, batch: 910, loss: 0.16281, acc: 0.86465\n",
      "global step 920, epoch: 1, batch: 920, loss: 0.19655, acc: 0.86551\n",
      "global step 930, epoch: 1, batch: 930, loss: 0.22448, acc: 0.86636\n",
      "global step 940, epoch: 1, batch: 940, loss: 0.14302, acc: 0.86720\n",
      "global step 950, epoch: 1, batch: 950, loss: 0.21709, acc: 0.86799\n",
      "global step 960, epoch: 1, batch: 960, loss: 0.22154, acc: 0.86880\n",
      "global step 970, epoch: 1, batch: 970, loss: 0.12073, acc: 0.86955\n",
      "global step 980, epoch: 1, batch: 980, loss: 0.14816, acc: 0.87034\n",
      "global step 990, epoch: 1, batch: 990, loss: 0.16782, acc: 0.87106\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.23103, acc: 0.87183\n",
      "global step 1010, epoch: 1, batch: 1010, loss: 0.18091, acc: 0.87258\n",
      "global step 1020, epoch: 1, batch: 1020, loss: 0.19917, acc: 0.87329\n",
      "global step 1030, epoch: 1, batch: 1030, loss: 0.14559, acc: 0.87400\n",
      "global step 1040, epoch: 1, batch: 1040, loss: 0.23991, acc: 0.87464\n",
      "global step 1050, epoch: 1, batch: 1050, loss: 0.15820, acc: 0.87533\n",
      "global step 1060, epoch: 1, batch: 1060, loss: 0.18217, acc: 0.87600\n",
      "global step 1070, epoch: 1, batch: 1070, loss: 0.19021, acc: 0.87662\n",
      "global step 1080, epoch: 1, batch: 1080, loss: 0.17161, acc: 0.87727\n",
      "global step 1090, epoch: 1, batch: 1090, loss: 0.16819, acc: 0.87789\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 0.19007, acc: 0.87851\n",
      "global step 1110, epoch: 1, batch: 1110, loss: 0.16328, acc: 0.87912\n",
      "global step 1120, epoch: 1, batch: 1120, loss: 0.18757, acc: 0.87972\n",
      "global step 1130, epoch: 1, batch: 1130, loss: 0.15525, acc: 0.88030\n",
      "global step 1140, epoch: 1, batch: 1140, loss: 0.15136, acc: 0.88093\n",
      "global step 1150, epoch: 1, batch: 1150, loss: 0.21118, acc: 0.88153\n",
      "global step 1160, epoch: 1, batch: 1160, loss: 0.14689, acc: 0.88211\n",
      "global step 1170, epoch: 1, batch: 1170, loss: 0.18136, acc: 0.88267\n",
      "global step 1180, epoch: 1, batch: 1180, loss: 0.16534, acc: 0.88322\n",
      "global step 1190, epoch: 1, batch: 1190, loss: 0.15712, acc: 0.88377\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.16808, acc: 0.88431\n",
      "global step 1210, epoch: 1, batch: 1210, loss: 0.15016, acc: 0.88482\n",
      "global step 1220, epoch: 1, batch: 1220, loss: 0.14191, acc: 0.88539\n",
      "global step 1230, epoch: 1, batch: 1230, loss: 0.16058, acc: 0.88588\n",
      "global step 1240, epoch: 1, batch: 1240, loss: 0.16594, acc: 0.88641\n",
      "global step 1250, epoch: 1, batch: 1250, loss: 0.14137, acc: 0.88691\n",
      "global step 1260, epoch: 1, batch: 1260, loss: 0.14944, acc: 0.88745\n",
      "global step 1270, epoch: 1, batch: 1270, loss: 0.13753, acc: 0.88797\n",
      "global step 1280, epoch: 1, batch: 1280, loss: 0.15907, acc: 0.88842\n",
      "global step 1290, epoch: 1, batch: 1290, loss: 0.16012, acc: 0.88882\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 0.14786, acc: 0.88928\n",
      "global step 1310, epoch: 1, batch: 1310, loss: 0.18807, acc: 0.88970\n",
      "global step 1320, epoch: 1, batch: 1320, loss: 0.16022, acc: 0.89013\n",
      "global step 1330, epoch: 1, batch: 1330, loss: 0.14039, acc: 0.89061\n",
      "global step 1340, epoch: 1, batch: 1340, loss: 0.19010, acc: 0.89104\n",
      "global step 1350, epoch: 1, batch: 1350, loss: 0.12983, acc: 0.89144\n",
      "global step 1360, epoch: 1, batch: 1360, loss: 0.17889, acc: 0.89186\n",
      "global step 1370, epoch: 1, batch: 1370, loss: 0.18550, acc: 0.89224\n",
      "global step 1380, epoch: 1, batch: 1380, loss: 0.18098, acc: 0.89268\n",
      "global step 1390, epoch: 1, batch: 1390, loss: 0.14672, acc: 0.89307\n",
      "eval loss: 0.12090, accu: 0.96108\n",
      "0.961075\n",
      "global step 1400, epoch: 2, batch: 9, loss: 0.14276, acc: 0.95870\n",
      "global step 1410, epoch: 2, batch: 19, loss: 0.11169, acc: 0.95728\n",
      "global step 1420, epoch: 2, batch: 29, loss: 0.13058, acc: 0.95793\n",
      "global step 1430, epoch: 2, batch: 39, loss: 0.18811, acc: 0.95744\n",
      "global step 1440, epoch: 2, batch: 49, loss: 0.15059, acc: 0.95793\n",
      "global step 1450, epoch: 2, batch: 59, loss: 0.12945, acc: 0.95831\n",
      "global step 1460, epoch: 2, batch: 69, loss: 0.14888, acc: 0.95816\n",
      "global step 1470, epoch: 2, batch: 79, loss: 0.14633, acc: 0.95802\n",
      "global step 1480, epoch: 2, batch: 89, loss: 0.14488, acc: 0.95766\n",
      "global step 1490, epoch: 2, batch: 99, loss: 0.13249, acc: 0.95768\n",
      "global step 1500, epoch: 2, batch: 109, loss: 0.13273, acc: 0.95772\n",
      "global step 1510, epoch: 2, batch: 119, loss: 0.12790, acc: 0.95741\n",
      "global step 1520, epoch: 2, batch: 129, loss: 0.06487, acc: 0.95752\n",
      "global step 1530, epoch: 2, batch: 139, loss: 0.11140, acc: 0.95766\n",
      "global step 1540, epoch: 2, batch: 149, loss: 0.11977, acc: 0.95756\n",
      "global step 1550, epoch: 2, batch: 159, loss: 0.12507, acc: 0.95759\n",
      "global step 1560, epoch: 2, batch: 169, loss: 0.14659, acc: 0.95750\n",
      "global step 1570, epoch: 2, batch: 179, loss: 0.15993, acc: 0.95763\n",
      "global step 1580, epoch: 2, batch: 189, loss: 0.11927, acc: 0.95769\n",
      "global step 1590, epoch: 2, batch: 199, loss: 0.16099, acc: 0.95771\n",
      "global step 1600, epoch: 2, batch: 209, loss: 0.13834, acc: 0.95763\n",
      "global step 1610, epoch: 2, batch: 219, loss: 0.09119, acc: 0.95791\n",
      "global step 1620, epoch: 2, batch: 229, loss: 0.13316, acc: 0.95795\n",
      "global step 1630, epoch: 2, batch: 239, loss: 0.14979, acc: 0.95798\n",
      "global step 1640, epoch: 2, batch: 249, loss: 0.10551, acc: 0.95810\n",
      "global step 1650, epoch: 2, batch: 259, loss: 0.14816, acc: 0.95806\n",
      "global step 1660, epoch: 2, batch: 269, loss: 0.12758, acc: 0.95820\n",
      "global step 1670, epoch: 2, batch: 279, loss: 0.14746, acc: 0.95814\n",
      "global step 1680, epoch: 2, batch: 289, loss: 0.11953, acc: 0.95815\n",
      "global step 1690, epoch: 2, batch: 299, loss: 0.09640, acc: 0.95821\n",
      "global step 1700, epoch: 2, batch: 309, loss: 0.15660, acc: 0.95820\n",
      "global step 1710, epoch: 2, batch: 319, loss: 0.15407, acc: 0.95821\n",
      "global step 1720, epoch: 2, batch: 329, loss: 0.16713, acc: 0.95801\n",
      "global step 1730, epoch: 2, batch: 339, loss: 0.15536, acc: 0.95805\n",
      "global step 1740, epoch: 2, batch: 349, loss: 0.11180, acc: 0.95809\n",
      "global step 1750, epoch: 2, batch: 359, loss: 0.11376, acc: 0.95814\n",
      "global step 1760, epoch: 2, batch: 369, loss: 0.11517, acc: 0.95811\n",
      "global step 1770, epoch: 2, batch: 379, loss: 0.10445, acc: 0.95826\n",
      "global step 1780, epoch: 2, batch: 389, loss: 0.12850, acc: 0.95831\n",
      "global step 1790, epoch: 2, batch: 399, loss: 0.12471, acc: 0.95823\n",
      "global step 1800, epoch: 2, batch: 409, loss: 0.15815, acc: 0.95821\n",
      "global step 1810, epoch: 2, batch: 419, loss: 0.11161, acc: 0.95817\n",
      "global step 1820, epoch: 2, batch: 429, loss: 0.12588, acc: 0.95807\n",
      "global step 1830, epoch: 2, batch: 439, loss: 0.11892, acc: 0.95805\n",
      "global step 1840, epoch: 2, batch: 449, loss: 0.16304, acc: 0.95800\n",
      "global step 1850, epoch: 2, batch: 459, loss: 0.16494, acc: 0.95796\n",
      "global step 1860, epoch: 2, batch: 469, loss: 0.12374, acc: 0.95800\n",
      "global step 1870, epoch: 2, batch: 479, loss: 0.09576, acc: 0.95808\n",
      "global step 1880, epoch: 2, batch: 489, loss: 0.13670, acc: 0.95808\n",
      "global step 1890, epoch: 2, batch: 499, loss: 0.12387, acc: 0.95797\n",
      "global step 1900, epoch: 2, batch: 509, loss: 0.11402, acc: 0.95791\n",
      "global step 1910, epoch: 2, batch: 519, loss: 0.19182, acc: 0.95793\n",
      "global step 1920, epoch: 2, batch: 529, loss: 0.08833, acc: 0.95791\n",
      "global step 1930, epoch: 2, batch: 539, loss: 0.13743, acc: 0.95787\n",
      "global step 1940, epoch: 2, batch: 549, loss: 0.14081, acc: 0.95791\n",
      "global step 1950, epoch: 2, batch: 559, loss: 0.13109, acc: 0.95791\n",
      "global step 1960, epoch: 2, batch: 569, loss: 0.12843, acc: 0.95797\n",
      "global step 1970, epoch: 2, batch: 579, loss: 0.09495, acc: 0.95799\n",
      "global step 1980, epoch: 2, batch: 589, loss: 0.11368, acc: 0.95805\n",
      "global step 1990, epoch: 2, batch: 599, loss: 0.10872, acc: 0.95804\n",
      "global step 2000, epoch: 2, batch: 609, loss: 0.12943, acc: 0.95806\n",
      "global step 2010, epoch: 2, batch: 619, loss: 0.10483, acc: 0.95812\n",
      "global step 2020, epoch: 2, batch: 629, loss: 0.13315, acc: 0.95814\n",
      "global step 2030, epoch: 2, batch: 639, loss: 0.12330, acc: 0.95823\n",
      "global step 2040, epoch: 2, batch: 649, loss: 0.10662, acc: 0.95827\n",
      "global step 2050, epoch: 2, batch: 659, loss: 0.11475, acc: 0.95827\n",
      "global step 2060, epoch: 2, batch: 669, loss: 0.09234, acc: 0.95829\n",
      "global step 2070, epoch: 2, batch: 679, loss: 0.15844, acc: 0.95829\n",
      "global step 2080, epoch: 2, batch: 689, loss: 0.07408, acc: 0.95837\n",
      "global step 2090, epoch: 2, batch: 699, loss: 0.16077, acc: 0.95831\n",
      "global step 2100, epoch: 2, batch: 709, loss: 0.12516, acc: 0.95828\n",
      "global step 2110, epoch: 2, batch: 719, loss: 0.13521, acc: 0.95828\n",
      "global step 2120, epoch: 2, batch: 729, loss: 0.12083, acc: 0.95834\n",
      "global step 2130, epoch: 2, batch: 739, loss: 0.11912, acc: 0.95838\n",
      "global step 2140, epoch: 2, batch: 749, loss: 0.13422, acc: 0.95846\n",
      "global step 2150, epoch: 2, batch: 759, loss: 0.13624, acc: 0.95843\n",
      "global step 2160, epoch: 2, batch: 769, loss: 0.12682, acc: 0.95842\n",
      "global step 2170, epoch: 2, batch: 779, loss: 0.14228, acc: 0.95841\n",
      "global step 2180, epoch: 2, batch: 789, loss: 0.10591, acc: 0.95841\n",
      "global step 2190, epoch: 2, batch: 799, loss: 0.10232, acc: 0.95844\n",
      "global step 2200, epoch: 2, batch: 809, loss: 0.09135, acc: 0.95847\n",
      "global step 2210, epoch: 2, batch: 819, loss: 0.12595, acc: 0.95847\n",
      "global step 2220, epoch: 2, batch: 829, loss: 0.11575, acc: 0.95844\n",
      "global step 2230, epoch: 2, batch: 839, loss: 0.09381, acc: 0.95847\n",
      "global step 2240, epoch: 2, batch: 849, loss: 0.10007, acc: 0.95851\n",
      "global step 2250, epoch: 2, batch: 859, loss: 0.13933, acc: 0.95850\n",
      "global step 2260, epoch: 2, batch: 869, loss: 0.13818, acc: 0.95852\n",
      "global step 2270, epoch: 2, batch: 879, loss: 0.11531, acc: 0.95849\n",
      "global step 2280, epoch: 2, batch: 889, loss: 0.08280, acc: 0.95847\n",
      "global step 2290, epoch: 2, batch: 899, loss: 0.16166, acc: 0.95846\n",
      "global step 2300, epoch: 2, batch: 909, loss: 0.10877, acc: 0.95847\n",
      "global step 2310, epoch: 2, batch: 919, loss: 0.11314, acc: 0.95847\n",
      "global step 2320, epoch: 2, batch: 929, loss: 0.13030, acc: 0.95847\n",
      "global step 2330, epoch: 2, batch: 939, loss: 0.11761, acc: 0.95846\n",
      "global step 2340, epoch: 2, batch: 949, loss: 0.13793, acc: 0.95854\n",
      "global step 2350, epoch: 2, batch: 959, loss: 0.09585, acc: 0.95855\n",
      "global step 2360, epoch: 2, batch: 969, loss: 0.07722, acc: 0.95858\n",
      "global step 2370, epoch: 2, batch: 979, loss: 0.11155, acc: 0.95860\n",
      "global step 2380, epoch: 2, batch: 989, loss: 0.12533, acc: 0.95860\n",
      "global step 2390, epoch: 2, batch: 999, loss: 0.11672, acc: 0.95862\n",
      "global step 2400, epoch: 2, batch: 1009, loss: 0.19287, acc: 0.95861\n",
      "global step 2410, epoch: 2, batch: 1019, loss: 0.13970, acc: 0.95861\n",
      "global step 2420, epoch: 2, batch: 1029, loss: 0.07679, acc: 0.95866\n",
      "global step 2430, epoch: 2, batch: 1039, loss: 0.11283, acc: 0.95868\n",
      "global step 2440, epoch: 2, batch: 1049, loss: 0.09943, acc: 0.95870\n",
      "global step 2450, epoch: 2, batch: 1059, loss: 0.11437, acc: 0.95872\n",
      "global step 2460, epoch: 2, batch: 1069, loss: 0.11945, acc: 0.95877\n",
      "global step 2470, epoch: 2, batch: 1079, loss: 0.11732, acc: 0.95875\n",
      "global step 2480, epoch: 2, batch: 1089, loss: 0.15520, acc: 0.95876\n",
      "global step 2490, epoch: 2, batch: 1099, loss: 0.14245, acc: 0.95880\n",
      "global step 2500, epoch: 2, batch: 1109, loss: 0.15477, acc: 0.95878\n",
      "global step 2510, epoch: 2, batch: 1119, loss: 0.12313, acc: 0.95881\n",
      "global step 2520, epoch: 2, batch: 1129, loss: 0.09243, acc: 0.95887\n",
      "global step 2530, epoch: 2, batch: 1139, loss: 0.09909, acc: 0.95887\n",
      "global step 2540, epoch: 2, batch: 1149, loss: 0.14399, acc: 0.95882\n",
      "global step 2550, epoch: 2, batch: 1159, loss: 0.11207, acc: 0.95886\n",
      "global step 2560, epoch: 2, batch: 1169, loss: 0.11692, acc: 0.95888\n",
      "global step 2570, epoch: 2, batch: 1179, loss: 0.11512, acc: 0.95889\n",
      "global step 2580, epoch: 2, batch: 1189, loss: 0.08530, acc: 0.95893\n",
      "global step 2590, epoch: 2, batch: 1199, loss: 0.12458, acc: 0.95897\n",
      "global step 2600, epoch: 2, batch: 1209, loss: 0.10509, acc: 0.95901\n",
      "global step 2610, epoch: 2, batch: 1219, loss: 0.11569, acc: 0.95899\n",
      "global step 2620, epoch: 2, batch: 1229, loss: 0.14816, acc: 0.95899\n",
      "global step 2630, epoch: 2, batch: 1239, loss: 0.10503, acc: 0.95900\n",
      "global step 2640, epoch: 2, batch: 1249, loss: 0.11021, acc: 0.95902\n",
      "global step 2650, epoch: 2, batch: 1259, loss: 0.16413, acc: 0.95900\n",
      "global step 2660, epoch: 2, batch: 1269, loss: 0.09050, acc: 0.95901\n",
      "global step 2670, epoch: 2, batch: 1279, loss: 0.17217, acc: 0.95902\n",
      "global step 2680, epoch: 2, batch: 1289, loss: 0.12965, acc: 0.95903\n",
      "global step 2690, epoch: 2, batch: 1299, loss: 0.11687, acc: 0.95905\n",
      "global step 2700, epoch: 2, batch: 1309, loss: 0.11645, acc: 0.95907\n",
      "global step 2710, epoch: 2, batch: 1319, loss: 0.09868, acc: 0.95909\n",
      "global step 2720, epoch: 2, batch: 1329, loss: 0.11456, acc: 0.95913\n",
      "global step 2730, epoch: 2, batch: 1339, loss: 0.10933, acc: 0.95916\n",
      "global step 2740, epoch: 2, batch: 1349, loss: 0.08431, acc: 0.95920\n",
      "global step 2750, epoch: 2, batch: 1359, loss: 0.10605, acc: 0.95920\n",
      "global step 2760, epoch: 2, batch: 1369, loss: 0.08533, acc: 0.95925\n",
      "global step 2770, epoch: 2, batch: 1379, loss: 0.12088, acc: 0.95926\n",
      "global step 2780, epoch: 2, batch: 1389, loss: 0.17584, acc: 0.95923\n",
      "eval loss: 0.08528, accu: 0.97185\n",
      "0.97185\n",
      "global step 2790, epoch: 3, batch: 8, loss: 0.12654, acc: 0.96833\n",
      "global step 2800, epoch: 3, batch: 18, loss: 0.07486, acc: 0.97065\n",
      "global step 2810, epoch: 3, batch: 28, loss: 0.12089, acc: 0.97101\n",
      "global step 2820, epoch: 3, batch: 38, loss: 0.07671, acc: 0.97083\n",
      "global step 2830, epoch: 3, batch: 48, loss: 0.07209, acc: 0.97115\n",
      "global step 2840, epoch: 3, batch: 58, loss: 0.09502, acc: 0.97121\n",
      "global step 2850, epoch: 3, batch: 68, loss: 0.12243, acc: 0.97037\n",
      "global step 2860, epoch: 3, batch: 78, loss: 0.09689, acc: 0.97017\n",
      "global step 2870, epoch: 3, batch: 88, loss: 0.10594, acc: 0.96996\n",
      "global step 2880, epoch: 3, batch: 98, loss: 0.11857, acc: 0.97020\n",
      "global step 2890, epoch: 3, batch: 108, loss: 0.06574, acc: 0.96968\n",
      "global step 2900, epoch: 3, batch: 118, loss: 0.06470, acc: 0.96986\n",
      "global step 2910, epoch: 3, batch: 128, loss: 0.08706, acc: 0.96997\n",
      "global step 2920, epoch: 3, batch: 138, loss: 0.07947, acc: 0.97007\n",
      "global step 2930, epoch: 3, batch: 148, loss: 0.09087, acc: 0.97009\n",
      "global step 2940, epoch: 3, batch: 158, loss: 0.06850, acc: 0.97013\n",
      "global step 2950, epoch: 3, batch: 168, loss: 0.09046, acc: 0.97019\n",
      "global step 2960, epoch: 3, batch: 178, loss: 0.07352, acc: 0.97009\n",
      "global step 2970, epoch: 3, batch: 188, loss: 0.07555, acc: 0.97016\n",
      "global step 2980, epoch: 3, batch: 198, loss: 0.08837, acc: 0.97015\n",
      "global step 2990, epoch: 3, batch: 208, loss: 0.17428, acc: 0.97002\n",
      "global step 3000, epoch: 3, batch: 218, loss: 0.09435, acc: 0.97021\n",
      "global step 3010, epoch: 3, batch: 228, loss: 0.08533, acc: 0.97021\n",
      "global step 3020, epoch: 3, batch: 238, loss: 0.13083, acc: 0.97021\n",
      "global step 3030, epoch: 3, batch: 248, loss: 0.07451, acc: 0.97013\n",
      "global step 3040, epoch: 3, batch: 258, loss: 0.08036, acc: 0.97008\n",
      "global step 3050, epoch: 3, batch: 268, loss: 0.10621, acc: 0.97001\n",
      "global step 3060, epoch: 3, batch: 278, loss: 0.11057, acc: 0.97004\n",
      "global step 3070, epoch: 3, batch: 288, loss: 0.09603, acc: 0.97003\n",
      "global step 3080, epoch: 3, batch: 298, loss: 0.09863, acc: 0.96998\n",
      "global step 3090, epoch: 3, batch: 308, loss: 0.09401, acc: 0.96995\n",
      "global step 3100, epoch: 3, batch: 318, loss: 0.08096, acc: 0.96999\n",
      "global step 3110, epoch: 3, batch: 328, loss: 0.08850, acc: 0.96990\n",
      "global step 3120, epoch: 3, batch: 338, loss: 0.07512, acc: 0.96995\n",
      "global step 3130, epoch: 3, batch: 348, loss: 0.06605, acc: 0.97002\n",
      "global step 3140, epoch: 3, batch: 358, loss: 0.08101, acc: 0.97003\n",
      "global step 3150, epoch: 3, batch: 368, loss: 0.11311, acc: 0.97001\n",
      "global step 3160, epoch: 3, batch: 378, loss: 0.10731, acc: 0.97002\n",
      "global step 3170, epoch: 3, batch: 388, loss: 0.11486, acc: 0.96997\n",
      "global step 3180, epoch: 3, batch: 398, loss: 0.07699, acc: 0.97001\n",
      "global step 3190, epoch: 3, batch: 408, loss: 0.09653, acc: 0.96981\n",
      "global step 3200, epoch: 3, batch: 418, loss: 0.11784, acc: 0.96982\n",
      "global step 3210, epoch: 3, batch: 428, loss: 0.08742, acc: 0.96987\n",
      "global step 3220, epoch: 3, batch: 438, loss: 0.10246, acc: 0.96980\n",
      "global step 3230, epoch: 3, batch: 448, loss: 0.08417, acc: 0.96979\n",
      "global step 3240, epoch: 3, batch: 458, loss: 0.11311, acc: 0.96982\n",
      "global step 3250, epoch: 3, batch: 468, loss: 0.14221, acc: 0.96977\n",
      "global step 3260, epoch: 3, batch: 478, loss: 0.08240, acc: 0.96976\n",
      "global step 3270, epoch: 3, batch: 488, loss: 0.08542, acc: 0.96982\n",
      "global step 3280, epoch: 3, batch: 498, loss: 0.06728, acc: 0.96980\n",
      "global step 3290, epoch: 3, batch: 508, loss: 0.09647, acc: 0.96978\n",
      "global step 3300, epoch: 3, batch: 518, loss: 0.08158, acc: 0.96978\n",
      "global step 3310, epoch: 3, batch: 528, loss: 0.10158, acc: 0.96978\n",
      "global step 3320, epoch: 3, batch: 538, loss: 0.09540, acc: 0.96973\n",
      "global step 3330, epoch: 3, batch: 548, loss: 0.08405, acc: 0.96975\n",
      "global step 3340, epoch: 3, batch: 558, loss: 0.09987, acc: 0.96973\n",
      "global step 3350, epoch: 3, batch: 568, loss: 0.09960, acc: 0.96967\n",
      "global step 3360, epoch: 3, batch: 578, loss: 0.12864, acc: 0.96965\n",
      "global step 3370, epoch: 3, batch: 588, loss: 0.08996, acc: 0.96971\n",
      "global step 3380, epoch: 3, batch: 598, loss: 0.11074, acc: 0.96972\n",
      "global step 3390, epoch: 3, batch: 608, loss: 0.08494, acc: 0.96965\n",
      "global step 3400, epoch: 3, batch: 618, loss: 0.12388, acc: 0.96964\n",
      "global step 3410, epoch: 3, batch: 628, loss: 0.07543, acc: 0.96959\n",
      "global step 3420, epoch: 3, batch: 638, loss: 0.08486, acc: 0.96956\n",
      "global step 3430, epoch: 3, batch: 648, loss: 0.08470, acc: 0.96959\n",
      "global step 3440, epoch: 3, batch: 658, loss: 0.08530, acc: 0.96954\n",
      "global step 3450, epoch: 3, batch: 668, loss: 0.08657, acc: 0.96956\n",
      "global step 3460, epoch: 3, batch: 678, loss: 0.10738, acc: 0.96959\n",
      "global step 3470, epoch: 3, batch: 688, loss: 0.09440, acc: 0.96960\n",
      "global step 3480, epoch: 3, batch: 698, loss: 0.07220, acc: 0.96965\n",
      "global step 3490, epoch: 3, batch: 708, loss: 0.07777, acc: 0.96965\n",
      "global step 3500, epoch: 3, batch: 718, loss: 0.10510, acc: 0.96964\n",
      "global step 3510, epoch: 3, batch: 728, loss: 0.09540, acc: 0.96966\n",
      "global step 3520, epoch: 3, batch: 738, loss: 0.12873, acc: 0.96961\n",
      "global step 3530, epoch: 3, batch: 748, loss: 0.12714, acc: 0.96957\n",
      "global step 3540, epoch: 3, batch: 758, loss: 0.09338, acc: 0.96962\n",
      "global step 3550, epoch: 3, batch: 768, loss: 0.08375, acc: 0.96963\n",
      "global step 3560, epoch: 3, batch: 778, loss: 0.07587, acc: 0.96969\n",
      "global step 3570, epoch: 3, batch: 788, loss: 0.08967, acc: 0.96969\n",
      "global step 3580, epoch: 3, batch: 798, loss: 0.07266, acc: 0.96964\n",
      "global step 3590, epoch: 3, batch: 808, loss: 0.14851, acc: 0.96960\n",
      "global step 3600, epoch: 3, batch: 818, loss: 0.07210, acc: 0.96958\n",
      "global step 3610, epoch: 3, batch: 828, loss: 0.10797, acc: 0.96958\n",
      "global step 3620, epoch: 3, batch: 838, loss: 0.12639, acc: 0.96955\n",
      "global step 3630, epoch: 3, batch: 848, loss: 0.08113, acc: 0.96953\n",
      "global step 3640, epoch: 3, batch: 858, loss: 0.06032, acc: 0.96956\n",
      "global step 3650, epoch: 3, batch: 868, loss: 0.09796, acc: 0.96958\n",
      "global step 3660, epoch: 3, batch: 878, loss: 0.08076, acc: 0.96955\n",
      "global step 3670, epoch: 3, batch: 888, loss: 0.05338, acc: 0.96953\n",
      "global step 3680, epoch: 3, batch: 898, loss: 0.10990, acc: 0.96950\n",
      "global step 3690, epoch: 3, batch: 908, loss: 0.11257, acc: 0.96949\n",
      "global step 3700, epoch: 3, batch: 918, loss: 0.14036, acc: 0.96946\n",
      "global step 3710, epoch: 3, batch: 928, loss: 0.09019, acc: 0.96946\n",
      "global step 3720, epoch: 3, batch: 938, loss: 0.10703, acc: 0.96945\n",
      "global step 3730, epoch: 3, batch: 948, loss: 0.10030, acc: 0.96946\n",
      "global step 3740, epoch: 3, batch: 958, loss: 0.12022, acc: 0.96944\n",
      "global step 3750, epoch: 3, batch: 968, loss: 0.11510, acc: 0.96946\n",
      "global step 3760, epoch: 3, batch: 978, loss: 0.10276, acc: 0.96950\n",
      "global step 3770, epoch: 3, batch: 988, loss: 0.08695, acc: 0.96950\n",
      "global step 3780, epoch: 3, batch: 998, loss: 0.07666, acc: 0.96950\n",
      "global step 3790, epoch: 3, batch: 1008, loss: 0.09534, acc: 0.96950\n",
      "global step 3800, epoch: 3, batch: 1018, loss: 0.09235, acc: 0.96951\n",
      "global step 3810, epoch: 3, batch: 1028, loss: 0.09663, acc: 0.96952\n",
      "global step 3820, epoch: 3, batch: 1038, loss: 0.09318, acc: 0.96953\n",
      "global step 3830, epoch: 3, batch: 1048, loss: 0.08114, acc: 0.96953\n",
      "global step 3840, epoch: 3, batch: 1058, loss: 0.09840, acc: 0.96948\n",
      "global step 3850, epoch: 3, batch: 1068, loss: 0.09667, acc: 0.96945\n",
      "global step 3860, epoch: 3, batch: 1078, loss: 0.13682, acc: 0.96942\n",
      "global step 3870, epoch: 3, batch: 1088, loss: 0.07505, acc: 0.96942\n",
      "global step 3880, epoch: 3, batch: 1098, loss: 0.09322, acc: 0.96945\n",
      "global step 3890, epoch: 3, batch: 1108, loss: 0.09268, acc: 0.96943\n",
      "global step 3900, epoch: 3, batch: 1118, loss: 0.07711, acc: 0.96943\n",
      "global step 3910, epoch: 3, batch: 1128, loss: 0.09516, acc: 0.96941\n",
      "global step 3920, epoch: 3, batch: 1138, loss: 0.11584, acc: 0.96939\n",
      "global step 3930, epoch: 3, batch: 1148, loss: 0.08381, acc: 0.96939\n",
      "global step 3940, epoch: 3, batch: 1158, loss: 0.09845, acc: 0.96939\n",
      "global step 3950, epoch: 3, batch: 1168, loss: 0.09486, acc: 0.96938\n",
      "global step 3960, epoch: 3, batch: 1178, loss: 0.12031, acc: 0.96936\n",
      "global step 3970, epoch: 3, batch: 1188, loss: 0.13081, acc: 0.96933\n",
      "global step 3980, epoch: 3, batch: 1198, loss: 0.10476, acc: 0.96932\n",
      "global step 3990, epoch: 3, batch: 1208, loss: 0.07510, acc: 0.96932\n",
      "global step 4000, epoch: 3, batch: 1218, loss: 0.09867, acc: 0.96929\n",
      "global step 4010, epoch: 3, batch: 1228, loss: 0.09016, acc: 0.96930\n",
      "global step 4020, epoch: 3, batch: 1238, loss: 0.09998, acc: 0.96931\n",
      "global step 4030, epoch: 3, batch: 1248, loss: 0.09994, acc: 0.96933\n",
      "global step 4040, epoch: 3, batch: 1258, loss: 0.07613, acc: 0.96933\n",
      "global step 4050, epoch: 3, batch: 1268, loss: 0.12026, acc: 0.96933\n",
      "global step 4060, epoch: 3, batch: 1278, loss: 0.10796, acc: 0.96932\n",
      "global step 4070, epoch: 3, batch: 1288, loss: 0.09190, acc: 0.96930\n",
      "global step 4080, epoch: 3, batch: 1298, loss: 0.07478, acc: 0.96932\n",
      "global step 4090, epoch: 3, batch: 1308, loss: 0.10411, acc: 0.96930\n",
      "global step 4100, epoch: 3, batch: 1318, loss: 0.10049, acc: 0.96928\n",
      "global step 4110, epoch: 3, batch: 1328, loss: 0.08785, acc: 0.96929\n",
      "global step 4120, epoch: 3, batch: 1338, loss: 0.11844, acc: 0.96930\n",
      "global step 4130, epoch: 3, batch: 1348, loss: 0.08999, acc: 0.96931\n",
      "global step 4140, epoch: 3, batch: 1358, loss: 0.07729, acc: 0.96931\n",
      "global step 4150, epoch: 3, batch: 1368, loss: 0.11550, acc: 0.96933\n",
      "global step 4160, epoch: 3, batch: 1378, loss: 0.08596, acc: 0.96934\n",
      "global step 4170, epoch: 3, batch: 1388, loss: 0.06562, acc: 0.96933\n",
      "eval loss: 0.05904, accu: 0.98104\n",
      "0.9810375\n",
      "global step 4180, epoch: 4, batch: 7, loss: 0.06359, acc: 0.98000\n",
      "global step 4190, epoch: 4, batch: 17, loss: 0.04762, acc: 0.98010\n",
      "global step 4200, epoch: 4, batch: 27, loss: 0.09665, acc: 0.98000\n",
      "global step 4210, epoch: 4, batch: 37, loss: 0.10184, acc: 0.97905\n",
      "global step 4220, epoch: 4, batch: 47, loss: 0.05751, acc: 0.97954\n",
      "global step 4230, epoch: 4, batch: 57, loss: 0.07236, acc: 0.97912\n",
      "global step 4240, epoch: 4, batch: 67, loss: 0.06835, acc: 0.97893\n",
      "global step 4250, epoch: 4, batch: 77, loss: 0.06915, acc: 0.97907\n",
      "global step 4260, epoch: 4, batch: 87, loss: 0.08088, acc: 0.97883\n",
      "global step 4270, epoch: 4, batch: 97, loss: 0.07419, acc: 0.97857\n",
      "global step 4280, epoch: 4, batch: 107, loss: 0.06496, acc: 0.97827\n",
      "global step 4290, epoch: 4, batch: 117, loss: 0.06472, acc: 0.97825\n",
      "global step 4300, epoch: 4, batch: 127, loss: 0.08652, acc: 0.97824\n",
      "global step 4310, epoch: 4, batch: 137, loss: 0.08165, acc: 0.97818\n",
      "global step 4320, epoch: 4, batch: 147, loss: 0.04968, acc: 0.97808\n",
      "global step 4330, epoch: 4, batch: 157, loss: 0.09035, acc: 0.97794\n",
      "global step 4340, epoch: 4, batch: 167, loss: 0.08062, acc: 0.97778\n",
      "global step 4350, epoch: 4, batch: 177, loss: 0.06104, acc: 0.97777\n",
      "global step 4360, epoch: 4, batch: 187, loss: 0.07051, acc: 0.97798\n",
      "global step 4370, epoch: 4, batch: 197, loss: 0.05038, acc: 0.97805\n",
      "global step 4380, epoch: 4, batch: 207, loss: 0.07676, acc: 0.97801\n",
      "global step 4390, epoch: 4, batch: 217, loss: 0.05401, acc: 0.97795\n",
      "global step 4400, epoch: 4, batch: 227, loss: 0.05569, acc: 0.97785\n",
      "global step 4410, epoch: 4, batch: 237, loss: 0.06050, acc: 0.97788\n",
      "global step 4420, epoch: 4, batch: 247, loss: 0.10414, acc: 0.97784\n",
      "global step 4430, epoch: 4, batch: 257, loss: 0.05508, acc: 0.97773\n",
      "global step 4440, epoch: 4, batch: 267, loss: 0.05654, acc: 0.97774\n",
      "global step 4450, epoch: 4, batch: 277, loss: 0.05088, acc: 0.97780\n",
      "global step 4460, epoch: 4, batch: 287, loss: 0.08263, acc: 0.97769\n",
      "global step 4470, epoch: 4, batch: 297, loss: 0.10954, acc: 0.97758\n",
      "global step 4480, epoch: 4, batch: 307, loss: 0.06517, acc: 0.97770\n",
      "global step 4490, epoch: 4, batch: 317, loss: 0.09652, acc: 0.97764\n",
      "global step 4500, epoch: 4, batch: 327, loss: 0.05583, acc: 0.97753\n",
      "global step 4510, epoch: 4, batch: 337, loss: 0.07246, acc: 0.97750\n",
      "global step 4520, epoch: 4, batch: 347, loss: 0.06893, acc: 0.97735\n",
      "global step 4530, epoch: 4, batch: 357, loss: 0.06183, acc: 0.97732\n",
      "global step 4540, epoch: 4, batch: 367, loss: 0.05207, acc: 0.97735\n",
      "global step 4550, epoch: 4, batch: 377, loss: 0.08050, acc: 0.97730\n",
      "global step 4560, epoch: 4, batch: 387, loss: 0.06373, acc: 0.97733\n",
      "global step 4570, epoch: 4, batch: 397, loss: 0.06523, acc: 0.97735\n",
      "global step 4580, epoch: 4, batch: 407, loss: 0.08047, acc: 0.97741\n",
      "global step 4590, epoch: 4, batch: 417, loss: 0.11108, acc: 0.97729\n",
      "global step 4600, epoch: 4, batch: 427, loss: 0.07642, acc: 0.97733\n",
      "global step 4610, epoch: 4, batch: 437, loss: 0.08222, acc: 0.97734\n",
      "global step 4620, epoch: 4, batch: 447, loss: 0.07940, acc: 0.97733\n",
      "global step 4630, epoch: 4, batch: 457, loss: 0.04712, acc: 0.97735\n",
      "global step 4640, epoch: 4, batch: 467, loss: 0.07546, acc: 0.97731\n",
      "global step 4650, epoch: 4, batch: 477, loss: 0.06953, acc: 0.97724\n",
      "global step 4660, epoch: 4, batch: 487, loss: 0.06016, acc: 0.97726\n",
      "global step 4670, epoch: 4, batch: 497, loss: 0.07144, acc: 0.97734\n",
      "global step 4680, epoch: 4, batch: 507, loss: 0.06618, acc: 0.97733\n",
      "global step 4690, epoch: 4, batch: 517, loss: 0.02762, acc: 0.97735\n",
      "global step 4700, epoch: 4, batch: 527, loss: 0.05917, acc: 0.97731\n",
      "global step 4710, epoch: 4, batch: 537, loss: 0.07008, acc: 0.97731\n",
      "global step 4720, epoch: 4, batch: 547, loss: 0.08418, acc: 0.97733\n",
      "global step 4730, epoch: 4, batch: 557, loss: 0.04201, acc: 0.97742\n",
      "global step 4740, epoch: 4, batch: 567, loss: 0.07065, acc: 0.97739\n",
      "global step 4750, epoch: 4, batch: 577, loss: 0.05751, acc: 0.97738\n",
      "global step 4760, epoch: 4, batch: 587, loss: 0.06873, acc: 0.97734\n",
      "global step 4770, epoch: 4, batch: 597, loss: 0.06607, acc: 0.97735\n",
      "global step 4780, epoch: 4, batch: 607, loss: 0.08826, acc: 0.97734\n",
      "global step 4790, epoch: 4, batch: 617, loss: 0.06687, acc: 0.97739\n",
      "global step 4800, epoch: 4, batch: 627, loss: 0.04660, acc: 0.97742\n",
      "global step 4810, epoch: 4, batch: 637, loss: 0.06103, acc: 0.97740\n",
      "global step 4820, epoch: 4, batch: 647, loss: 0.07365, acc: 0.97743\n",
      "global step 4830, epoch: 4, batch: 657, loss: 0.06158, acc: 0.97745\n",
      "global step 4840, epoch: 4, batch: 667, loss: 0.08183, acc: 0.97742\n",
      "global step 4850, epoch: 4, batch: 677, loss: 0.08325, acc: 0.97737\n",
      "global step 4860, epoch: 4, batch: 687, loss: 0.08727, acc: 0.97737\n",
      "global step 4870, epoch: 4, batch: 697, loss: 0.05608, acc: 0.97732\n",
      "global step 4880, epoch: 4, batch: 707, loss: 0.06638, acc: 0.97730\n",
      "global step 4890, epoch: 4, batch: 717, loss: 0.05078, acc: 0.97732\n",
      "global step 4900, epoch: 4, batch: 727, loss: 0.08038, acc: 0.97732\n",
      "global step 4910, epoch: 4, batch: 737, loss: 0.07666, acc: 0.97730\n",
      "global step 4920, epoch: 4, batch: 747, loss: 0.07620, acc: 0.97725\n",
      "global step 4930, epoch: 4, batch: 757, loss: 0.04195, acc: 0.97727\n",
      "global step 4940, epoch: 4, batch: 767, loss: 0.07953, acc: 0.97722\n",
      "global step 4950, epoch: 4, batch: 777, loss: 0.06826, acc: 0.97723\n",
      "global step 4960, epoch: 4, batch: 787, loss: 0.05206, acc: 0.97725\n",
      "global step 4970, epoch: 4, batch: 797, loss: 0.07778, acc: 0.97725\n",
      "global step 4980, epoch: 4, batch: 807, loss: 0.08703, acc: 0.97724\n",
      "global step 4990, epoch: 4, batch: 817, loss: 0.08732, acc: 0.97723\n",
      "global step 5000, epoch: 4, batch: 827, loss: 0.04283, acc: 0.97724\n",
      "global step 5010, epoch: 4, batch: 837, loss: 0.06606, acc: 0.97718\n",
      "global step 5020, epoch: 4, batch: 847, loss: 0.08281, acc: 0.97717\n",
      "global step 5030, epoch: 4, batch: 857, loss: 0.07879, acc: 0.97716\n",
      "global step 5040, epoch: 4, batch: 867, loss: 0.07932, acc: 0.97718\n",
      "global step 5050, epoch: 4, batch: 877, loss: 0.07859, acc: 0.97719\n",
      "global step 5060, epoch: 4, batch: 887, loss: 0.08469, acc: 0.97717\n",
      "global step 5070, epoch: 4, batch: 897, loss: 0.07578, acc: 0.97719\n",
      "global step 5080, epoch: 4, batch: 907, loss: 0.05559, acc: 0.97715\n",
      "global step 5090, epoch: 4, batch: 917, loss: 0.07389, acc: 0.97717\n",
      "global step 5100, epoch: 4, batch: 927, loss: 0.05720, acc: 0.97717\n",
      "global step 5110, epoch: 4, batch: 937, loss: 0.09012, acc: 0.97718\n",
      "global step 5120, epoch: 4, batch: 947, loss: 0.06490, acc: 0.97715\n",
      "global step 5130, epoch: 4, batch: 957, loss: 0.05143, acc: 0.97715\n",
      "global step 5140, epoch: 4, batch: 967, loss: 0.04446, acc: 0.97718\n",
      "global step 5150, epoch: 4, batch: 977, loss: 0.11478, acc: 0.97717\n",
      "global step 5160, epoch: 4, batch: 987, loss: 0.06480, acc: 0.97716\n",
      "global step 5170, epoch: 4, batch: 997, loss: 0.06484, acc: 0.97716\n",
      "global step 5180, epoch: 4, batch: 1007, loss: 0.05580, acc: 0.97716\n",
      "global step 5190, epoch: 4, batch: 1017, loss: 0.05240, acc: 0.97716\n",
      "global step 5200, epoch: 4, batch: 1027, loss: 0.05856, acc: 0.97718\n",
      "global step 5210, epoch: 4, batch: 1037, loss: 0.06796, acc: 0.97719\n",
      "global step 5220, epoch: 4, batch: 1047, loss: 0.06396, acc: 0.97719\n",
      "global step 5230, epoch: 4, batch: 1057, loss: 0.06634, acc: 0.97720\n",
      "global step 5240, epoch: 4, batch: 1067, loss: 0.07456, acc: 0.97720\n",
      "global step 5250, epoch: 4, batch: 1077, loss: 0.09822, acc: 0.97722\n",
      "global step 5260, epoch: 4, batch: 1087, loss: 0.07990, acc: 0.97721\n",
      "global step 5270, epoch: 4, batch: 1097, loss: 0.05262, acc: 0.97722\n",
      "global step 5280, epoch: 4, batch: 1107, loss: 0.08237, acc: 0.97721\n",
      "global step 5290, epoch: 4, batch: 1117, loss: 0.08311, acc: 0.97721\n",
      "global step 5300, epoch: 4, batch: 1127, loss: 0.06980, acc: 0.97721\n",
      "global step 5310, epoch: 4, batch: 1137, loss: 0.06665, acc: 0.97726\n",
      "global step 5320, epoch: 4, batch: 1147, loss: 0.09746, acc: 0.97725\n",
      "global step 5330, epoch: 4, batch: 1157, loss: 0.07077, acc: 0.97725\n",
      "global step 5340, epoch: 4, batch: 1167, loss: 0.06270, acc: 0.97724\n",
      "global step 5360, epoch: 4, batch: 1187, loss: 0.04689, acc: 0.97729\n",
      "global step 5370, epoch: 4, batch: 1197, loss: 0.06582, acc: 0.97728\n",
      "global step 5380, epoch: 4, batch: 1207, loss: 0.07986, acc: 0.97729\n",
      "global step 5390, epoch: 4, batch: 1217, loss: 0.04684, acc: 0.97731\n",
      "global step 5400, epoch: 4, batch: 1227, loss: 0.06496, acc: 0.97733\n",
      "global step 5410, epoch: 4, batch: 1237, loss: 0.07235, acc: 0.97730\n",
      "global step 5420, epoch: 4, batch: 1247, loss: 0.08163, acc: 0.97730\n",
      "global step 5430, epoch: 4, batch: 1257, loss: 0.08867, acc: 0.97730\n",
      "global step 5440, epoch: 4, batch: 1267, loss: 0.08674, acc: 0.97730\n",
      "global step 5450, epoch: 4, batch: 1277, loss: 0.08354, acc: 0.97729\n",
      "global step 5460, epoch: 4, batch: 1287, loss: 0.06519, acc: 0.97730\n",
      "global step 5470, epoch: 4, batch: 1297, loss: 0.07042, acc: 0.97731\n",
      "global step 5480, epoch: 4, batch: 1307, loss: 0.04129, acc: 0.97729\n",
      "global step 5490, epoch: 4, batch: 1317, loss: 0.08213, acc: 0.97727\n",
      "global step 5500, epoch: 4, batch: 1327, loss: 0.04223, acc: 0.97730\n",
      "global step 5510, epoch: 4, batch: 1337, loss: 0.05886, acc: 0.97730\n",
      "global step 5520, epoch: 4, batch: 1347, loss: 0.08529, acc: 0.97730\n",
      "global step 5530, epoch: 4, batch: 1357, loss: 0.04957, acc: 0.97732\n",
      "global step 5540, epoch: 4, batch: 1367, loss: 0.06740, acc: 0.97734\n",
      "global step 5550, epoch: 4, batch: 1377, loss: 0.06318, acc: 0.97733\n",
      "global step 5560, epoch: 4, batch: 1387, loss: 0.04966, acc: 0.97734\n",
      "eval loss: 0.04425, accu: 0.98621\n",
      "0.9862125\n",
      "global step 5570, epoch: 5, batch: 6, loss: 0.05071, acc: 0.98222\n",
      "global step 5580, epoch: 5, batch: 16, loss: 0.04996, acc: 0.98323\n",
      "global step 5590, epoch: 5, batch: 26, loss: 0.04934, acc: 0.98295\n",
      "global step 5600, epoch: 5, batch: 36, loss: 0.03967, acc: 0.98296\n",
      "global step 5610, epoch: 5, batch: 46, loss: 0.07094, acc: 0.98283\n",
      "global step 5620, epoch: 5, batch: 56, loss: 0.03027, acc: 0.98310\n",
      "global step 5630, epoch: 5, batch: 66, loss: 0.04988, acc: 0.98326\n",
      "global step 5640, epoch: 5, batch: 76, loss: 0.05360, acc: 0.98325\n",
      "global step 5650, epoch: 5, batch: 86, loss: 0.04061, acc: 0.98302\n",
      "global step 5660, epoch: 5, batch: 96, loss: 0.04610, acc: 0.98307\n",
      "global step 5670, epoch: 5, batch: 106, loss: 0.04092, acc: 0.98292\n",
      "global step 5680, epoch: 5, batch: 116, loss: 0.06682, acc: 0.98292\n",
      "global step 5690, epoch: 5, batch: 126, loss: 0.05847, acc: 0.98296\n",
      "global step 5700, epoch: 5, batch: 136, loss: 0.06143, acc: 0.98315\n",
      "global step 5710, epoch: 5, batch: 146, loss: 0.04398, acc: 0.98313\n",
      "global step 5720, epoch: 5, batch: 156, loss: 0.07029, acc: 0.98304\n",
      "global step 5730, epoch: 5, batch: 166, loss: 0.01897, acc: 0.98308\n",
      "global step 5740, epoch: 5, batch: 176, loss: 0.04497, acc: 0.98313\n",
      "global step 5750, epoch: 5, batch: 186, loss: 0.06761, acc: 0.98306\n",
      "global step 5760, epoch: 5, batch: 196, loss: 0.05376, acc: 0.98310\n",
      "global step 5770, epoch: 5, batch: 206, loss: 0.06806, acc: 0.98300\n",
      "global step 5780, epoch: 5, batch: 216, loss: 0.03516, acc: 0.98304\n",
      "global step 5790, epoch: 5, batch: 226, loss: 0.06622, acc: 0.98305\n",
      "global step 5800, epoch: 5, batch: 236, loss: 0.03182, acc: 0.98316\n",
      "global step 5810, epoch: 5, batch: 246, loss: 0.05868, acc: 0.98315\n",
      "global step 5820, epoch: 5, batch: 256, loss: 0.07370, acc: 0.98313\n",
      "global step 5830, epoch: 5, batch: 266, loss: 0.04981, acc: 0.98318\n",
      "global step 5840, epoch: 5, batch: 276, loss: 0.03228, acc: 0.98321\n",
      "global step 5850, epoch: 5, batch: 286, loss: 0.04136, acc: 0.98322\n",
      "global step 5860, epoch: 5, batch: 296, loss: 0.05876, acc: 0.98322\n",
      "global step 5870, epoch: 5, batch: 306, loss: 0.05800, acc: 0.98318\n",
      "global step 5880, epoch: 5, batch: 316, loss: 0.03281, acc: 0.98316\n",
      "global step 5890, epoch: 5, batch: 326, loss: 0.04483, acc: 0.98313\n",
      "global step 5900, epoch: 5, batch: 336, loss: 0.05160, acc: 0.98318\n",
      "global step 5910, epoch: 5, batch: 346, loss: 0.04522, acc: 0.98320\n",
      "global step 5920, epoch: 5, batch: 356, loss: 0.03787, acc: 0.98319\n",
      "global step 5930, epoch: 5, batch: 366, loss: 0.05751, acc: 0.98316\n",
      "global step 5940, epoch: 5, batch: 376, loss: 0.07342, acc: 0.98315\n",
      "global step 5950, epoch: 5, batch: 386, loss: 0.06615, acc: 0.98312\n",
      "global step 5960, epoch: 5, batch: 396, loss: 0.02461, acc: 0.98318\n",
      "global step 5970, epoch: 5, batch: 406, loss: 0.06509, acc: 0.98320\n",
      "global step 5980, epoch: 5, batch: 416, loss: 0.05319, acc: 0.98319\n",
      "global step 5990, epoch: 5, batch: 426, loss: 0.07250, acc: 0.98318\n",
      "global step 6000, epoch: 5, batch: 436, loss: 0.03005, acc: 0.98317\n",
      "global step 6010, epoch: 5, batch: 446, loss: 0.05900, acc: 0.98321\n",
      "global step 6020, epoch: 5, batch: 456, loss: 0.04344, acc: 0.98318\n",
      "global step 6030, epoch: 5, batch: 466, loss: 0.05288, acc: 0.98319\n",
      "global step 6040, epoch: 5, batch: 476, loss: 0.04689, acc: 0.98322\n",
      "global step 6050, epoch: 5, batch: 486, loss: 0.06826, acc: 0.98318\n",
      "global step 6060, epoch: 5, batch: 496, loss: 0.04103, acc: 0.98319\n",
      "global step 6070, epoch: 5, batch: 506, loss: 0.04877, acc: 0.98324\n",
      "global step 6080, epoch: 5, batch: 516, loss: 0.02973, acc: 0.98328\n",
      "global step 6090, epoch: 5, batch: 526, loss: 0.03784, acc: 0.98328\n",
      "global step 6100, epoch: 5, batch: 536, loss: 0.05197, acc: 0.98332\n",
      "global step 6110, epoch: 5, batch: 546, loss: 0.02115, acc: 0.98331\n",
      "global step 6120, epoch: 5, batch: 556, loss: 0.06178, acc: 0.98332\n",
      "global step 6130, epoch: 5, batch: 566, loss: 0.05842, acc: 0.98330\n",
      "global step 6140, epoch: 5, batch: 576, loss: 0.04806, acc: 0.98334\n",
      "global step 6150, epoch: 5, batch: 586, loss: 0.06380, acc: 0.98325\n",
      "global step 6160, epoch: 5, batch: 596, loss: 0.04509, acc: 0.98322\n",
      "global step 6170, epoch: 5, batch: 606, loss: 0.05957, acc: 0.98315\n",
      "global step 6180, epoch: 5, batch: 616, loss: 0.05311, acc: 0.98311\n",
      "global step 6190, epoch: 5, batch: 626, loss: 0.05430, acc: 0.98314\n",
      "global step 6200, epoch: 5, batch: 636, loss: 0.04337, acc: 0.98318\n",
      "global step 6210, epoch: 5, batch: 646, loss: 0.05090, acc: 0.98318\n",
      "global step 6220, epoch: 5, batch: 656, loss: 0.05545, acc: 0.98315\n",
      "global step 6230, epoch: 5, batch: 666, loss: 0.05107, acc: 0.98318\n",
      "global step 6240, epoch: 5, batch: 676, loss: 0.04173, acc: 0.98321\n",
      "global step 6250, epoch: 5, batch: 686, loss: 0.06141, acc: 0.98325\n",
      "global step 6260, epoch: 5, batch: 696, loss: 0.04258, acc: 0.98327\n",
      "global step 6270, epoch: 5, batch: 706, loss: 0.02736, acc: 0.98327\n",
      "global step 6280, epoch: 5, batch: 716, loss: 0.04321, acc: 0.98327\n",
      "global step 6290, epoch: 5, batch: 726, loss: 0.04540, acc: 0.98328\n",
      "global step 6300, epoch: 5, batch: 736, loss: 0.08157, acc: 0.98329\n",
      "global step 6310, epoch: 5, batch: 746, loss: 0.04680, acc: 0.98329\n",
      "global step 6320, epoch: 5, batch: 756, loss: 0.06146, acc: 0.98327\n",
      "global step 6330, epoch: 5, batch: 766, loss: 0.04295, acc: 0.98330\n",
      "global step 6340, epoch: 5, batch: 776, loss: 0.06717, acc: 0.98331\n",
      "global step 6350, epoch: 5, batch: 786, loss: 0.05998, acc: 0.98331\n",
      "global step 6360, epoch: 5, batch: 796, loss: 0.05524, acc: 0.98332\n",
      "global step 6370, epoch: 5, batch: 806, loss: 0.05935, acc: 0.98337\n",
      "global step 6380, epoch: 5, batch: 816, loss: 0.02881, acc: 0.98335\n",
      "global step 6390, epoch: 5, batch: 826, loss: 0.03375, acc: 0.98334\n",
      "global step 6400, epoch: 5, batch: 836, loss: 0.03946, acc: 0.98335\n",
      "global step 6410, epoch: 5, batch: 846, loss: 0.03352, acc: 0.98337\n",
      "global step 6420, epoch: 5, batch: 856, loss: 0.05623, acc: 0.98336\n",
      "global step 6430, epoch: 5, batch: 866, loss: 0.03927, acc: 0.98337\n",
      "global step 6440, epoch: 5, batch: 876, loss: 0.02124, acc: 0.98337\n",
      "global step 6450, epoch: 5, batch: 886, loss: 0.09448, acc: 0.98332\n",
      "global step 6460, epoch: 5, batch: 896, loss: 0.03803, acc: 0.98334\n",
      "global step 6470, epoch: 5, batch: 906, loss: 0.04020, acc: 0.98333\n",
      "global step 6480, epoch: 5, batch: 916, loss: 0.03889, acc: 0.98334\n",
      "global step 6490, epoch: 5, batch: 926, loss: 0.03177, acc: 0.98338\n",
      "global step 6500, epoch: 5, batch: 936, loss: 0.05481, acc: 0.98336\n",
      "global step 6510, epoch: 5, batch: 946, loss: 0.07390, acc: 0.98335\n",
      "global step 6520, epoch: 5, batch: 956, loss: 0.02991, acc: 0.98333\n",
      "global step 6530, epoch: 5, batch: 966, loss: 0.04970, acc: 0.98332\n",
      "global step 6540, epoch: 5, batch: 976, loss: 0.04722, acc: 0.98331\n",
      "global step 6550, epoch: 5, batch: 986, loss: 0.06344, acc: 0.98330\n",
      "global step 6560, epoch: 5, batch: 996, loss: 0.04336, acc: 0.98332\n",
      "global step 6570, epoch: 5, batch: 1006, loss: 0.05378, acc: 0.98330\n",
      "global step 6580, epoch: 5, batch: 1016, loss: 0.05678, acc: 0.98331\n",
      "global step 6590, epoch: 5, batch: 1026, loss: 0.05478, acc: 0.98334\n",
      "global step 6600, epoch: 5, batch: 1036, loss: 0.07583, acc: 0.98331\n",
      "global step 6610, epoch: 5, batch: 1046, loss: 0.03885, acc: 0.98331\n",
      "global step 6620, epoch: 5, batch: 1056, loss: 0.04509, acc: 0.98332\n",
      "global step 6630, epoch: 5, batch: 1066, loss: 0.04020, acc: 0.98333\n",
      "global step 6640, epoch: 5, batch: 1076, loss: 0.04105, acc: 0.98329\n",
      "global step 6650, epoch: 5, batch: 1086, loss: 0.03823, acc: 0.98326\n",
      "global step 6660, epoch: 5, batch: 1096, loss: 0.04540, acc: 0.98324\n",
      "global step 6670, epoch: 5, batch: 1106, loss: 0.05857, acc: 0.98323\n",
      "global step 6680, epoch: 5, batch: 1116, loss: 0.05704, acc: 0.98325\n",
      "global step 6690, epoch: 5, batch: 1126, loss: 0.06355, acc: 0.98324\n",
      "global step 6700, epoch: 5, batch: 1136, loss: 0.04219, acc: 0.98324\n",
      "global step 6710, epoch: 5, batch: 1146, loss: 0.04007, acc: 0.98325\n",
      "global step 6720, epoch: 5, batch: 1156, loss: 0.03096, acc: 0.98326\n",
      "global step 6730, epoch: 5, batch: 1166, loss: 0.05203, acc: 0.98327\n",
      "global step 6740, epoch: 5, batch: 1176, loss: 0.04021, acc: 0.98328\n",
      "global step 6750, epoch: 5, batch: 1186, loss: 0.03213, acc: 0.98327\n",
      "global step 6760, epoch: 5, batch: 1196, loss: 0.04008, acc: 0.98328\n",
      "global step 6770, epoch: 5, batch: 1206, loss: 0.04570, acc: 0.98328\n",
      "global step 6780, epoch: 5, batch: 1216, loss: 0.05222, acc: 0.98330\n",
      "global step 6790, epoch: 5, batch: 1226, loss: 0.03503, acc: 0.98331\n",
      "global step 6800, epoch: 5, batch: 1236, loss: 0.04879, acc: 0.98331\n",
      "global step 6810, epoch: 5, batch: 1246, loss: 0.03693, acc: 0.98331\n",
      "global step 6820, epoch: 5, batch: 1256, loss: 0.05778, acc: 0.98332\n",
      "global step 6830, epoch: 5, batch: 1266, loss: 0.03924, acc: 0.98331\n",
      "global step 6840, epoch: 5, batch: 1276, loss: 0.04740, acc: 0.98332\n",
      "global step 6850, epoch: 5, batch: 1286, loss: 0.03314, acc: 0.98331\n",
      "global step 6860, epoch: 5, batch: 1296, loss: 0.04406, acc: 0.98333\n",
      "global step 6870, epoch: 5, batch: 1306, loss: 0.03333, acc: 0.98335\n",
      "global step 6880, epoch: 5, batch: 1316, loss: 0.05381, acc: 0.98335\n",
      "global step 6890, epoch: 5, batch: 1326, loss: 0.06000, acc: 0.98334\n",
      "global step 6900, epoch: 5, batch: 1336, loss: 0.08279, acc: 0.98334\n",
      "global step 6910, epoch: 5, batch: 1346, loss: 0.07137, acc: 0.98334\n",
      "global step 6920, epoch: 5, batch: 1356, loss: 0.04639, acc: 0.98335\n",
      "global step 6930, epoch: 5, batch: 1366, loss: 0.05452, acc: 0.98334\n",
      "global step 6940, epoch: 5, batch: 1376, loss: 0.05665, acc: 0.98335\n",
      "global step 6950, epoch: 5, batch: 1386, loss: 0.03586, acc: 0.98335\n",
      "eval loss: 0.03534, accu: 0.98926\n",
      "0.9892625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-19 11:55:54,915] [    INFO] - tokenizer config file saved in checkpoint/tokenizer_config.json\n",
      "[2023-01-19 11:55:54,919] [    INFO] - Special tokens file saved in checkpoint/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('checkpoint/tokenizer_config.json',\n",
       " 'checkpoint/special_tokens_map.json',\n",
       " 'checkpoint/added_tokens.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型训练：\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "save_dir = \"checkpoint\"\n",
    "if not  os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "pre_accu=0\n",
    "accu=0\n",
    "global_step = 0\n",
    "print(\"重新开始\")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0 :\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "    # 每轮结束对验证集进行评估\n",
    "    accu = evaluate(model, criterion, metric, dev_data_loader)\n",
    "    print(accu)\n",
    "    if accu > pre_accu:\n",
    "        # 保存较上一轮效果更优的模型参数\n",
    "        save_param_path = os.path.join(save_dir, 'model_state.pdparams')  # 保存模型参数\n",
    "        paddle.save(model.state_dict(), save_param_path)\n",
    "        pre_accu=accu\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from checkpoint/model_state.pdparams\n"
     ]
    }
   ],
   "source": [
    "# 加载在验证集上效果最优的一轮的模型参数\n",
    "import os\n",
    "import paddle\n",
    "\n",
    "params_path = 'checkpoint/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.03723, accu: 0.98846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9884625"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试最优模型参数在验证集上的分数\n",
    "evaluate(model, criterion, metric, dev_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型预测函数\n",
    "def predict(model, data, tokenizer, label_map, batch_size=1):\n",
    "    examples = []\n",
    "    # 将输入数据（list格式）处理为模型可接受的格式\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            max_seq_length=128,\n",
    "            is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    # Seperates data into some batches.\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        # The last batch whose size is less than the config batch_size setting.\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [label_map[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    return results  # 返回预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '科技', 1: '体育', 2: '时政', 3: '股票', 4: '娱乐', 5: '教育', 6: '家居', 7: '财经', 8: '房产', 9: '社会', 10: '游戏', 11: '彩票', 12: '星座', 13: '时尚'}\n"
     ]
    }
   ],
   "source": [
    "# 定义要进行分类的类别\n",
    "label_list=list(train.label.unique())\n",
    "label_map = { \n",
    "    idx: label_text for idx, label_text in enumerate(label_list)\n",
    "}\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取要进行预测的测试集文件\n",
    "test = pd.read_csv('./test.csv',sep='\\t')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义对数据的预处理函数,处理为模型输入指定list格式\n",
    "def preprocess_prediction_data(data):\n",
    "    examples = []\n",
    "    for text_a in data:\n",
    "        examples.append({\"text_a\": text_a})\n",
    "    return examples\n",
    "\n",
    "# 对测试集数据进行格式处理\n",
    "data1 = list(test.text_a)\n",
    "examples = preprocess_prediction_data(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对测试集进行预测\n",
    "results = predict(model, examples, tokenizer, label_map, batch_size=16)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将list格式的预测结果存储为txt文件，提交格式要求：每行一个类别\n",
    "def write_results(labels, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf8\") as f:\n",
    "        f.writelines(\"\\n\".join(labels))\n",
    "\n",
    "write_results(results, \"./result.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: result.txt (deflated 89%)\r\n"
     ]
    }
   ],
   "source": [
    "# 因格式要求为zip，故需要将结果文件压缩为submission.zip提交文件\n",
    "!zip 'submission.zip' 'result.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移动data目录下提交结果文件至主目录下，便于结果文件的保存\n",
    "!cp -r /home/aistudio/data/data103654/submission.zip /home/aistudio/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 伪标签法\n",
    "\n",
    "伪标签法个人是采用了取多模型预测相同的部分，以下为其实现演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/work/结果融合\n"
     ]
    }
   ],
   "source": [
    "# 进入结果融合目录，该文件夹主要预测结果文件\n",
    "%cd /home/aistudio/work/结果融合/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mergesim.py:54: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  x[~df1['label'].isin(['label'])]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 伪标签法演示（取多个预测结果中预测全部相同的部分出来），具体参数可以查看源文件进行修改\n",
    "# 运行完成后可以得到newtest3.csv文件即伪标签数据，得到该数据文件后可以将其加入到模型训练中去从而增大训练数据量提升模型效果\n",
    "!python mergesim.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
